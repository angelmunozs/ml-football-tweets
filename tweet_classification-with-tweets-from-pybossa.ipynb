{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet classification\n",
    "\n",
    "File names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_location = './'\n",
    "\n",
    "# To read\n",
    "tweets_raw_file = base_location + 'footballsentiment_task.csv'\n",
    "tweets_run_file = base_location + 'footballsentiment_task_run.csv'\n",
    "corpus_tweets_2012_xml = base_location + 'general-train-tagged-3l.xml'\n",
    "corpus_tweets_2017_xml = base_location + 'intertass-train-tagged.xml'\n",
    "\n",
    "# To generate\n",
    "corpus_tweets_2012_csv = base_location + 'general-train-tagged-3l.csv'\n",
    "corpus_tweets_2017_csv = base_location + 'intertass-train-tagged.csv'\n",
    "corpus_tweets_csv = base_location + 'corpus_tweets.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 411\n",
      "Evaluated tweets so far: 177\n"
     ]
    }
   ],
   "source": [
    "tweets_raw = pd.read_csv(tweets_raw_file, encoding='utf-8')\n",
    "tweets_run = pd.read_csv(tweets_run_file, encoding='utf-8')\n",
    "\n",
    "print('Total tweets: %d' % len(tweets_raw))\n",
    "print('Evaluated tweets so far: %d' % len(tweets_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test data\n",
    "\n",
    "Aux function to convert evaluations to numeric values, accordnig to the rule:\n",
    "- `Negativo`: -1.\n",
    "- `Positivo`: 1.\n",
    "- `Neutro`: 0.\n",
    "- Anything else: 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_numeric(evaluation):\n",
    "    if evaluation == 'Positivo':\n",
    "        return 1\n",
    "    elif evaluation == 'Neutro':\n",
    "        return 0\n",
    "    elif evaluation == 'Negativo':\n",
    "        return -1\n",
    "    else:\n",
    "        return 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build new array of dictionaries with keys `id` (the task ID), `tweet` (the tweet string) and `score` (the tweet evaluation) by joining data from both CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total different tweets evalauted so far: 588\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary of tweets where key is the task__id\n",
    "tweets_obj = {}\n",
    "for index, row in tweets_raw.iterrows():\n",
    "    tweets_obj[row.task__id] = row.taskinfo__Tweet\n",
    "\n",
    "# Build dictinary of tweet scores where key is the task_id\n",
    "scores_obj = {}\n",
    "for index, row in tweets_run.iterrows():\n",
    "    scores_obj[row.task_run__task_id] = row.task_run__info\n",
    "\n",
    "# Create final wteets dictionary\n",
    "own_tweets = []\n",
    "for i, key in enumerate(scores_obj):\n",
    "    own_tweets.append({\n",
    "        'id': key,\n",
    "        'tweet': tweets_obj[key], \n",
    "        'score': convert_to_numeric(scores_obj[key])\n",
    "    })\n",
    "\n",
    "print('Total different tweets evalauted so far: %d' % len(own_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "Import libraries to read XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import objectify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import/read most recent corpus (2017):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 values of sentiment: N, P, NONE, NEU\n",
    "xml = objectify.parse(open(corpus_tweets_2017_xml))\n",
    "root = xml.getroot()\n",
    "general_tweets_corpus_train_2017 = pd.DataFrame(columns=('content', 'polarity'))\n",
    "tweets = root.getchildren()\n",
    "for i in range(0, len(tweets)):\n",
    "    tweet = tweets[i]\n",
    "    row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiment.polarity.value.text]))\n",
    "    row_s = pd.Series(row)\n",
    "    row_s.name = i\n",
    "    general_tweets_corpus_train_2017 = general_tweets_corpus_train_2017.append(row_s)\n",
    "    \n",
    "general_tweets_corpus_train_2017.to_csv(corpus_tweets_2017_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import/read biggest corpus (2012), to concatenate it with the previous noe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 values of sentiment: N, P, NONE, NEU\n",
    "xml = objectify.parse(open(corpus_tweets_2012_xml))\n",
    "root = xml.getroot()\n",
    "general_tweets_corpus_train_2012 = pd.DataFrame(columns=('content', 'polarity'))\n",
    "tweets = root.getchildren()\n",
    "for i in range(0, len(tweets)):\n",
    "    tweet = tweets[i]\n",
    "    row = dict(zip(['content', 'polarity'], [tweet.content.text, tweet.sentiments.polarity.value.text]))\n",
    "    row_s = pd.Series(row)\n",
    "    row_s.name = i\n",
    "    general_tweets_corpus_train_2012 = general_tweets_corpus_train_2012.append(row_s)\n",
    "    \n",
    "general_tweets_corpus_train_2012.to_csv(corpus_tweets_2012_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate general corpus dataset with 2017 one, to have a better result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Cruz está que de sale RT \"@agusperezblanco: Mi...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>“@VaquerBoy: @David_Busta Recuerda que no eres...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>Tres años de instrucción, uno y medio de suspe...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>Preguntan a Valcárcel si aún quiere devolver c...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>Cameron rechaza reformar el Tratado de Lisboa ...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>Opel ha sido, es y será empresa estratégica pa...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>¿\"Agresión con cámara fotográfica\"? Cuesta muc...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6583</th>\n",
       "      <td>El Supremo juzga a Otegui. repasamos los que h...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6279</th>\n",
       "      <td>Telefónica contrata a marido d la Vicep. del G...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>Ya está aqui Bern Shcuster!!!;)) http://t.co/5...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content polarity\n",
       "472   Cruz está que de sale RT \"@agusperezblanco: Mi...        P\n",
       "6175  “@VaquerBoy: @David_Busta Recuerda que no eres...        P\n",
       "4932  Tres años de instrucción, uno y medio de suspe...        N\n",
       "3095  Preguntan a Valcárcel si aún quiere devolver c...      NEU\n",
       "352   Cameron rechaza reformar el Tratado de Lisboa ...      NEU\n",
       "5670  Opel ha sido, es y será empresa estratégica pa...        P\n",
       "613   ¿\"Agresión con cámara fotográfica\"? Cuesta muc...        N\n",
       "6583  El Supremo juzga a Otegui. repasamos los que h...      NEU\n",
       "6279  Telefónica contrata a marido d la Vicep. del G...        N\n",
       "2435  Ya está aqui Bern Shcuster!!!;)) http://t.co/5...        P"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus = pd.concat([\n",
    "        general_tweets_corpus_train_2012,\n",
    "        general_tweets_corpus_train_2017\n",
    "    ])\n",
    "tweets_corpus.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total corpus tweets: 6605\n"
     ]
    }
   ],
   "source": [
    "print('Total corpus tweets: %d' % len(tweets_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove tweets without polarity (polarity `NONE`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_corpus = tweets_corpus.query('polarity != \"NONE\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "url_regex = re.compile('https?:\\/\\/t\\.co\\/[\\w]{8,8}')\n",
    "tweets_corpus_no_links = tweets_corpus\n",
    "tweets_corpus_no_links['content'] = tweets_corpus_no_links['content'].map(lambda x: re.sub(url_regex, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total corpus tweets after cleaning: 6605\n"
     ]
    }
   ],
   "source": [
    "print('Total corpus tweets after cleaning: %d' % len(tweets_corpus_no_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>@PabloAIglesias no se si mañana o pasado,jaja</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>#Andalucia necesita claridad en sus cuentas, t...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>El Gobierno recuerda a sindicatos y empresario...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6233</th>\n",
       "      <td>Según el relato de EP la única presión que se ...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>Si Chacón gana es urgente que aprenda la difer...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>Es lo maravilloso de este nuevo medio de expre...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Resumen de la larga y agria noche en Bruselas:...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658</th>\n",
       "      <td>Bruselas será ligeramente flexible con los obj...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6951</th>\n",
       "      <td>Desde que en julio de 1988 empecé a trabajar e...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>Hoy en Cartaya con las personas mayores. Nos m...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content polarity\n",
       "1109      @PabloAIglesias no se si mañana o pasado,jaja        P\n",
       "2783  #Andalucia necesita claridad en sus cuentas, t...        N\n",
       "2263  El Gobierno recuerda a sindicatos y empresario...        N\n",
       "6233  Según el relato de EP la única presión que se ...        N\n",
       "3562  Si Chacón gana es urgente que aprenda la difer...      NEU\n",
       "1284  Es lo maravilloso de este nuevo medio de expre...        P\n",
       "330   Resumen de la larga y agria noche en Bruselas:...        N\n",
       "4658  Bruselas será ligeramente flexible con los obj...        N\n",
       "6951  Desde que en julio de 1988 empecé a trabajar e...        N\n",
       "1240  Hoy en Cartaya con las personas mayores. Nos m...        P"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/david.santosg/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#download spanish stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "spanish_stopwords = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¿',\n",
       " '¡',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#we add spanish punctuation\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n",
    "non_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer       \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Conda.io/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 1    0.484784\n",
       "-1    0.393641\n",
       " 0    0.121575\n",
       "Name: polarity_bin, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus_no_links['polarity_bin'] = 0\n",
    "tweets_corpus_no_links.polarity_bin[tweets_corpus_no_links.polarity.isin(['P'])] = 1\n",
    "tweets_corpus_no_links.polarity_bin[tweets_corpus_no_links.polarity.isin(['N'])] = -1\n",
    "tweets_corpus_no_links.polarity_bin.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibLookupError",
     "evalue": "JoblibLookupError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/opt/Conda.io/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7fb1655f6270, file \"/...3.6/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__pycache__/__main__.cpython-36.pyc', '__doc__': None, '__file__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/Conda.i.../python3.6/site-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7fb1655f6270, file \"/...3.6/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__pycache__/__main__.cpython-36.pyc', '__doc__': None, '__file__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/Conda.i.../python3.6/site-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2018-05-17T16:49:58.111162', 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'session': '6FDAF978638049498EA7C7B55ACF455B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'6FDAF978638049498EA7C7B55ACF455B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2018-05-17T16:49:58.111162', 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'session': '6FDAF978638049498EA7C7B55ACF455B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'6FDAF978638049498EA7C7B55ACF455B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2018-05-17T16:49:58.111162', 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'session': '6FDAF978638049498EA7C7B55ACF455B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-64-bbf023576d52>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fb115a232e8, executi..._before_exec=None error_in_exec=None result=None>)\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n   2826                 code = compiler(mod, cell_name, \"single\")\n-> 2827                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fb115a92270, file \"<ipython-input-64-bbf023576d52>\", line 27>\n        result = <ExecutionResult object at 7fb115a232e8, executi..._before_exec=None error_in_exec=None result=None>\n   2828                     return True\n   2829 \n   2830             # Flush softspace\n   2831             if softspace(sys.stdout, 0):\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fb115a92270, file \"<ipython-input-64-bbf023576d52>\", line 27>, result=<ExecutionResult object at 7fb115a232e8, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fb115a92270, file \"<ipython-input-64-bbf023576d52>\", line 27>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"base_location = '../../ml-football-tweets/'\\n\\n# T..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", \"base_location = './ml-football-tweets/'\\n\\n# To re..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"base_location = './'\\n\\n# To read\\ntweets_raw_file ..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", 'def convert_to_numeric(evaluation):\\n    if evalu...\\n        return -1\\n    else:\\n        return 2    ', \"# Build dictionary of tweets where key is the ta...t tweets evalauted so far: %d' % len(own_tweets))\", 'from lxml import objectify', \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2012_csv, index=False, encoding='utf-8')\", 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', \"print('Total corpus tweets: %d' % len(tweets_corpus))\", 'tweets_corpus = tweets_corpus.query(\\'polarity != \"NONE\"\\')', \"tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {15:                                                 ...o que estoy viendo a los Reyes... Per...      NEU, 16:                                                 ...y me tiran a la basura\" http://t.co/j...        N, 21:                                                 ...n irresponsble         P\n\n[1680 rows x 2 columns], 22:                                                 ...trle derecho  ...        N\n\n[94 rows x 2 columns], 24:                                               co...              #progrmscmbidos cc d juste        N, 25:                                                 ...n 5 minutos de retrso... Feliz Cumple...      NEU, 26:                                                 ...o no dije que jessic se provechr de m...        N, 34:                                                 ...o no dije que jessic se provechr de m...        N, 37:                                                 ...o no dije que jessic se provechr de m...        N, 39:                                                 ...o no dije que jessic se provechr de m...        N, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, '_':  1    0.484784\n-1    0.393641\n 0    0.121575\nName: polarity_bin, dtype: float64, '_15':                                                 ...o que estoy viendo a los Reyes... Per...      NEU, '_16':                                                 ...y me tiran a la basura\" http://t.co/j...        N, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"base_location = '../../ml-football-tweets/'\\n\\n# T..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", \"base_location = './ml-football-tweets/'\\n\\n# To re..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"base_location = './'\\n\\n# To read\\ntweets_raw_file ..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", 'def convert_to_numeric(evaluation):\\n    if evalu...\\n        return -1\\n    else:\\n        return 2    ', \"# Build dictionary of tweets where key is the ta...t tweets evalauted so far: %d' % len(own_tweets))\", 'from lxml import objectify', \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2012_csv, index=False, encoding='utf-8')\", 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', \"print('Total corpus tweets: %d' % len(tweets_corpus))\", 'tweets_corpus = tweets_corpus.query(\\'polarity != \"NONE\"\\')', \"tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {15:                                                 ...o que estoy viendo a los Reyes... Per...      NEU, 16:                                                 ...y me tiran a la basura\" http://t.co/j...        N, 21:                                                 ...n irresponsble         P\n\n[1680 rows x 2 columns], 22:                                                 ...trle derecho  ...        N\n\n[94 rows x 2 columns], 24:                                               co...              #progrmscmbidos cc d juste        N, 25:                                                 ...n 5 minutos de retrso... Feliz Cumple...      NEU, 26:                                                 ...o no dije que jessic se provechr de m...        N, 34:                                                 ...o no dije que jessic se provechr de m...        N, 37:                                                 ...o no dije que jessic se provechr de m...        N, 39:                                                 ...o no dije que jessic se provechr de m...        N, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, '_':  1    0.484784\n-1    0.393641\n 0    0.121575\nName: polarity_bin, dtype: float64, '_15':                                                 ...o que estoy viendo a los Reyes... Per...      NEU, '_16':                                                 ...y me tiran a la basura\" http://t.co/j...        N, ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/david.santosg/ml-football-tweets/<ipython-input-64-bbf023576d52> in <module>()\n     22     'cls__max_iter': (500, 1000)\n     23 }\n     24 \n     25 \n     26 grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n---> 27 grid_search.fit(tweets_corpus_no_links.content, tweets_corpus_no_links.polarity_bin)\n     28 \n     29 \n     30 \n     31 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ..._score=True,\n       scoring='roc_auc', verbose=0), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...score=True,\n       scoring='roc_auc', verbose=0)>\n        X = 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object\n        y = 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        groups = None\n        self.param_grid = {'cls__C': (0.2, 0.5, 0.7), 'cls__loss': ('hinge', 'squared_hinge'), 'cls__max_iter': (500, 1000), 'vect__max_df': (0.5, 1.9), 'vect__max_features': (500, 1000), 'vect__min_df': (10, 20, 50), 'vect__ngram_range': ((1, 1), (1, 2))}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ..._score=True,\n       scoring='roc_auc', verbose=0), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nLookupError                                        Thu May 17 16:49:58 2018\nPID: 6995                            Python 3.6.0: /opt/Conda.io/bin/python\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([2010, 2014, 2017, ..., 6602, 6603, 6604]), test=array([   0,    1,    2, ..., 2417, 2419, 2445]), verbose=0, parameters={'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...2', random_state=None, tol=0.0001, verbose=0))])>\n        X_train = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y_train = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...2', random_state=None, tol=0.0001, verbose=0))])>\n        X = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        transform.fit_transform = <bound method CountVectorizer.fit_transform of C...on tokenize at 0x7fb115ae6e18>, vocabulary=None)>\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params_steps = {'cls': {}, 'vect': {}}\n        name = 'vect'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64)\n    834         max_df = self.max_df\n    835         min_df = self.min_df\n    836         max_features = self.max_features\n    837 \n    838         vocabulary, X = self._count_vocab(raw_documents,\n--> 839                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    840 \n    841         if self.binary:\n    842             X.data.fill(1)\n    843 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, fixed_vocab=False)\n    757         indptr = _make_int_array()\n    758         values = _make_int_array()\n    759         indptr.append(0)\n    760         for doc in raw_documents:\n    761             feature_counter = {}\n--> 762             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    763                 try:\n    764                     feature_idx = vocabulary[feature]\n    765                     if feature_idx not in feature_counter:\n    766                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc='La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne')\n    236         elif self.analyzer == 'word':\n    237             stop_words = self.get_stop_words()\n    238             tokenize = self.build_tokenizer()\n    239 \n    240             return lambda doc: self._word_ngrams(\n--> 241                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    242 \n    243         else:\n    244             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    245                              self.analyzer)\n\n...........................................................................\n/home/david.santosg/ml-football-tweets/<ipython-input-59-ba382de6c55a> in tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne')\n     12 \n     13 def tokenize(text):\n     14     # remove non letters\n     15     text = ''.join([c for c in text if c not in non_words])\n     16     # tokenize\n---> 17     tokens =  word_tokenize(text)\n     18 \n     19     # stem\n     20     try:\n     21         stems = stem_tokens(tokens, stemmer)\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in word_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n    104     for the specified language).\n    105 \n    106     :param text: text to split into sentences\n    107     :param language: the model name in the Punkt corpus\n    108     \"\"\"\n--> 109     return [token for sent in sent_tokenize(text, language)\n        text = 'la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne'\n        language = 'english'\n    110             for token in _treebank_word_tokenize(sent)]\n    111 \n    112 \n    113 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in sent_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n     88     for the specified language).\n     89 \n     90     :param text: text to split into sentences\n     91     :param language: the model name in the Punkt corpus\n     92     \"\"\"\n---> 93     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n        tokenizer = undefined\n        language = 'english'\n     94     return tokenizer.tokenize(text)\n     95 \n     96 # Standard word tokenizer.\n     97 _treebank_word_tokenize = TreebankWordTokenizer().tokenize\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in load(resource_url='nltk:tokenizers/punkt/PY3/english.pickle', format='pickle', cache=True, verbose=False, logic_parser=None, fstruct_reader=None, encoding=None)\n    803     # Let the user know what's going on.\n    804     if verbose:\n    805         print('<<Loading %s>>' % (resource_url,))\n    806 \n    807     # Load the resource.\n--> 808     opened_resource = _open(resource_url)\n        opened_resource = undefined\n        resource_url = 'nltk:tokenizers/punkt/PY3/english.pickle'\n    809 \n    810     if format == 'raw':\n    811         resource_val = opened_resource.read()\n    812     elif format == 'pickle':\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in _open(resource_url='nltk:tokenizers/punkt/PY3/english.pickle')\n    921     \"\"\"\n    922     resource_url = normalize_resource_url(resource_url)\n    923     protocol, path_ = split_resource_url(resource_url)\n    924 \n    925     if protocol is None or protocol.lower() == 'nltk':\n--> 926         return find(path_, path + ['']).open()\n        path_ = 'tokenizers/punkt/PY3/english.pickle'\n    927     elif protocol.lower() == 'file':\n    928         # urllib might not use mode='rb', so handle this one ourselves:\n    929         return find(path_, ['']).open()\n    930     else:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in find(resource_name='tokenizers/punkt/PY3/english.pickle', paths=['/home/david.santosg/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', ''])\n    643         (resource_name,), initial_indent='  ', subsequent_indent='  ',\n    644         width=66)\n    645     msg += '\\n  Searched in:' + ''.join('\\n    - %r' % d for d in paths)\n    646     sep = '*' * 70\n    647     resource_not_found = '\\n%s\\n%s\\n%s' % (sep, msg, sep)\n--> 648     raise LookupError(resource_not_found)\n        resource_not_found = '\\n***********************************************...*************************************************'\n    649 \n    650 \n    651 def retrieve(resource_url, filename=None, verbose=True):\n    652     \"\"\"\n\nLookupError: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 238, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py\", line 268, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py\", line 234, in _fit\n    Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 839, in fit_transform\n    self.fixed_vocabulary_)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 762, in _count_vocab\n    for feature in analyze(doc):\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 241, in <lambda>\n    tokenize(preprocess(self.decode(doc))), stop_words)\n  File \"<ipython-input-59-ba382de6c55a>\", line 17, in tokenize\n    tokens =  word_tokenize(text)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 109, in word_tokenize\n    return [token for sent in sent_tokenize(text, language)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 93, in sent_tokenize\n    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n  File \"/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py\", line 808, in load\n    opened_resource = _open(resource_url)\n  File \"/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py\", line 926, in _open\n    return find(path_, path + ['']).open()\n  File \"/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py\", line 648, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/Conda.io/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nLookupError                                        Thu May 17 16:49:58 2018\nPID: 6995                            Python 3.6.0: /opt/Conda.io/bin/python\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([2010, 2014, 2017, ..., 6602, 6603, 6604]), test=array([   0,    1,    2, ..., 2417, 2419, 2445]), verbose=0, parameters={'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...2', random_state=None, tol=0.0001, verbose=0))])>\n        X_train = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y_train = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...2', random_state=None, tol=0.0001, verbose=0))])>\n        X = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        transform.fit_transform = <bound method CountVectorizer.fit_transform of C...on tokenize at 0x7fb115ae6e18>, vocabulary=None)>\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params_steps = {'cls': {}, 'vect': {}}\n        name = 'vect'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64)\n    834         max_df = self.max_df\n    835         min_df = self.min_df\n    836         max_features = self.max_features\n    837 \n    838         vocabulary, X = self._count_vocab(raw_documents,\n--> 839                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    840 \n    841         if self.binary:\n    842             X.data.fill(1)\n    843 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, fixed_vocab=False)\n    757         indptr = _make_int_array()\n    758         values = _make_int_array()\n    759         indptr.append(0)\n    760         for doc in raw_documents:\n    761             feature_counter = {}\n--> 762             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    763                 try:\n    764                     feature_idx = vocabulary[feature]\n    765                     if feature_idx not in feature_counter:\n    766                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc='La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne')\n    236         elif self.analyzer == 'word':\n    237             stop_words = self.get_stop_words()\n    238             tokenize = self.build_tokenizer()\n    239 \n    240             return lambda doc: self._word_ngrams(\n--> 241                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    242 \n    243         else:\n    244             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    245                              self.analyzer)\n\n...........................................................................\n/home/david.santosg/ml-football-tweets/<ipython-input-59-ba382de6c55a> in tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne')\n     12 \n     13 def tokenize(text):\n     14     # remove non letters\n     15     text = ''.join([c for c in text if c not in non_words])\n     16     # tokenize\n---> 17     tokens =  word_tokenize(text)\n     18 \n     19     # stem\n     20     try:\n     21         stems = stem_tokens(tokens, stemmer)\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in word_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n    104     for the specified language).\n    105 \n    106     :param text: text to split into sentences\n    107     :param language: the model name in the Punkt corpus\n    108     \"\"\"\n--> 109     return [token for sent in sent_tokenize(text, language)\n        text = 'la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne'\n        language = 'english'\n    110             for token in _treebank_word_tokenize(sent)]\n    111 \n    112 \n    113 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in sent_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n     88     for the specified language).\n     89 \n     90     :param text: text to split into sentences\n     91     :param language: the model name in the Punkt corpus\n     92     \"\"\"\n---> 93     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n        tokenizer = undefined\n        language = 'english'\n     94     return tokenizer.tokenize(text)\n     95 \n     96 # Standard word tokenizer.\n     97 _treebank_word_tokenize = TreebankWordTokenizer().tokenize\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in load(resource_url='nltk:tokenizers/punkt/PY3/english.pickle', format='pickle', cache=True, verbose=False, logic_parser=None, fstruct_reader=None, encoding=None)\n    803     # Let the user know what's going on.\n    804     if verbose:\n    805         print('<<Loading %s>>' % (resource_url,))\n    806 \n    807     # Load the resource.\n--> 808     opened_resource = _open(resource_url)\n        opened_resource = undefined\n        resource_url = 'nltk:tokenizers/punkt/PY3/english.pickle'\n    809 \n    810     if format == 'raw':\n    811         resource_val = opened_resource.read()\n    812     elif format == 'pickle':\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in _open(resource_url='nltk:tokenizers/punkt/PY3/english.pickle')\n    921     \"\"\"\n    922     resource_url = normalize_resource_url(resource_url)\n    923     protocol, path_ = split_resource_url(resource_url)\n    924 \n    925     if protocol is None or protocol.lower() == 'nltk':\n--> 926         return find(path_, path + ['']).open()\n        path_ = 'tokenizers/punkt/PY3/english.pickle'\n    927     elif protocol.lower() == 'file':\n    928         # urllib might not use mode='rb', so handle this one ourselves:\n    929         return find(path_, ['']).open()\n    930     else:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in find(resource_name='tokenizers/punkt/PY3/english.pickle', paths=['/home/david.santosg/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', ''])\n    643         (resource_name,), initial_indent='  ', subsequent_indent='  ',\n    644         width=66)\n    645     msg += '\\n  Searched in:' + ''.join('\\n    - %r' % d for d in paths)\n    646     sep = '*' * 70\n    647     resource_not_found = '\\n%s\\n%s\\n%s' % (sep, msg, sep)\n--> 648     raise LookupError(resource_not_found)\n        resource_not_found = '\\n***********************************************...*************************************************'\n    649 \n    650 \n    651 def retrieve(resource_url, filename=None, verbose=True):\n    652     \"\"\"\n\nLookupError: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nLookupError                                        Thu May 17 16:49:58 2018\nPID: 6995                            Python 3.6.0: /opt/Conda.io/bin/python\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([2010, 2014, 2017, ..., 6602, 6603, 6604]), test=array([   0,    1,    2, ..., 2417, 2419, 2445]), verbose=0, parameters={'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...2', random_state=None, tol=0.0001, verbose=0))])>\n        X_train = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y_train = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...2', random_state=None, tol=0.0001, verbose=0))])>\n        X = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        transform.fit_transform = <bound method CountVectorizer.fit_transform of C...on tokenize at 0x7fb115ae6e18>, vocabulary=None)>\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params_steps = {'cls': {}, 'vect': {}}\n        name = 'vect'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64)\n    834         max_df = self.max_df\n    835         min_df = self.min_df\n    836         max_features = self.max_features\n    837 \n    838         vocabulary, X = self._count_vocab(raw_documents,\n--> 839                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    840 \n    841         if self.binary:\n    842             X.data.fill(1)\n    843 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, fixed_vocab=False)\n    757         indptr = _make_int_array()\n    758         values = _make_int_array()\n    759         indptr.append(0)\n    760         for doc in raw_documents:\n    761             feature_counter = {}\n--> 762             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    763                 try:\n    764                     feature_idx = vocabulary[feature]\n    765                     if feature_idx not in feature_counter:\n    766                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc='La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne')\n    236         elif self.analyzer == 'word':\n    237             stop_words = self.get_stop_words()\n    238             tokenize = self.build_tokenizer()\n    239 \n    240             return lambda doc: self._word_ngrams(\n--> 241                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    242 \n    243         else:\n    244             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    245                              self.analyzer)\n\n...........................................................................\n/home/david.santosg/ml-football-tweets/<ipython-input-59-ba382de6c55a> in tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne')\n     12 \n     13 def tokenize(text):\n     14     # remove non letters\n     15     text = ''.join([c for c in text if c not in non_words])\n     16     # tokenize\n---> 17     tokens =  word_tokenize(text)\n     18 \n     19     # stem\n     20     try:\n     21         stems = stem_tokens(tokens, stemmer)\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in word_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n    104     for the specified language).\n    105 \n    106     :param text: text to split into sentences\n    107     :param language: the model name in the Punkt corpus\n    108     \"\"\"\n--> 109     return [token for sent in sent_tokenize(text, language)\n        text = 'la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne'\n        language = 'english'\n    110             for token in _treebank_word_tokenize(sent)]\n    111 \n    112 \n    113 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in sent_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n     88     for the specified language).\n     89 \n     90     :param text: text to split into sentences\n     91     :param language: the model name in the Punkt corpus\n     92     \"\"\"\n---> 93     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n        tokenizer = undefined\n        language = 'english'\n     94     return tokenizer.tokenize(text)\n     95 \n     96 # Standard word tokenizer.\n     97 _treebank_word_tokenize = TreebankWordTokenizer().tokenize\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in load(resource_url='nltk:tokenizers/punkt/PY3/english.pickle', format='pickle', cache=True, verbose=False, logic_parser=None, fstruct_reader=None, encoding=None)\n    803     # Let the user know what's going on.\n    804     if verbose:\n    805         print('<<Loading %s>>' % (resource_url,))\n    806 \n    807     # Load the resource.\n--> 808     opened_resource = _open(resource_url)\n        opened_resource = undefined\n        resource_url = 'nltk:tokenizers/punkt/PY3/english.pickle'\n    809 \n    810     if format == 'raw':\n    811         resource_val = opened_resource.read()\n    812     elif format == 'pickle':\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in _open(resource_url='nltk:tokenizers/punkt/PY3/english.pickle')\n    921     \"\"\"\n    922     resource_url = normalize_resource_url(resource_url)\n    923     protocol, path_ = split_resource_url(resource_url)\n    924 \n    925     if protocol is None or protocol.lower() == 'nltk':\n--> 926         return find(path_, path + ['']).open()\n        path_ = 'tokenizers/punkt/PY3/english.pickle'\n    927     elif protocol.lower() == 'file':\n    928         # urllib might not use mode='rb', so handle this one ourselves:\n    929         return find(path_, ['']).open()\n    930     else:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in find(resource_name='tokenizers/punkt/PY3/english.pickle', paths=['/home/david.santosg/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', ''])\n    643         (resource_name,), initial_indent='  ', subsequent_indent='  ',\n    644         width=66)\n    645     msg += '\\n  Searched in:' + ''.join('\\n    - %r' % d for d in paths)\n    646     sep = '*' * 70\n    647     resource_not_found = '\\n%s\\n%s\\n%s' % (sep, msg, sep)\n--> 648     raise LookupError(resource_not_found)\n        resource_not_found = '\\n***********************************************...*************************************************'\n    649 \n    650 \n    651 def retrieve(resource_url, filename=None, verbose=True):\n    652     \"\"\"\n\nLookupError: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibLookupError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-bbf023576d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_corpus_no_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_corpus_no_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibLookupError\u001b[0m: JoblibLookupError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/opt/Conda.io/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7fb1655f6270, file \"/...3.6/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__pycache__/__main__.cpython-36.pyc', '__doc__': None, '__file__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/Conda.i.../python3.6/site-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7fb1655f6270, file \"/...3.6/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__pycache__/__main__.cpython-36.pyc', '__doc__': None, '__file__': '/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.6/site-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/Conda.i.../python3.6/site-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2018-05-17T16:49:58.111162', 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'session': '6FDAF978638049498EA7C7B55ACF455B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'6FDAF978638049498EA7C7B55ACF455B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2018-05-17T16:49:58.111162', 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'session': '6FDAF978638049498EA7C7B55ACF455B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'6FDAF978638049498EA7C7B55ACF455B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': '2018-05-17T16:49:58.111162', 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'session': '6FDAF978638049498EA7C7B55ACF455B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '813927BE18694FAB9D33E69E1C291C8C', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from sklearn.model_selection import GridSearchCV...nks.content, tweets_corpus_no_links.polarity_bin)', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-64-bbf023576d52>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fb115a232e8, executi..._before_exec=None error_in_exec=None result=None>)\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n   2826                 code = compiler(mod, cell_name, \"single\")\n-> 2827                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fb115a92270, file \"<ipython-input-64-bbf023576d52>\", line 27>\n        result = <ExecutionResult object at 7fb115a232e8, executi..._before_exec=None error_in_exec=None result=None>\n   2828                     return True\n   2829 \n   2830             # Flush softspace\n   2831             if softspace(sys.stdout, 0):\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fb115a92270, file \"<ipython-input-64-bbf023576d52>\", line 27>, result=<ExecutionResult object at 7fb115a232e8, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fb115a92270, file \"<ipython-input-64-bbf023576d52>\", line 27>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"base_location = '../../ml-football-tweets/'\\n\\n# T..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", \"base_location = './ml-football-tweets/'\\n\\n# To re..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"base_location = './'\\n\\n# To read\\ntweets_raw_file ..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", 'def convert_to_numeric(evaluation):\\n    if evalu...\\n        return -1\\n    else:\\n        return 2    ', \"# Build dictionary of tweets where key is the ta...t tweets evalauted so far: %d' % len(own_tweets))\", 'from lxml import objectify', \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2012_csv, index=False, encoding='utf-8')\", 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', \"print('Total corpus tweets: %d' % len(tweets_corpus))\", 'tweets_corpus = tweets_corpus.query(\\'polarity != \"NONE\"\\')', \"tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {15:                                                 ...o que estoy viendo a los Reyes... Per...      NEU, 16:                                                 ...y me tiran a la basura\" http://t.co/j...        N, 21:                                                 ...n irresponsble         P\n\n[1680 rows x 2 columns], 22:                                                 ...trle derecho  ...        N\n\n[94 rows x 2 columns], 24:                                               co...              #progrmscmbidos cc d juste        N, 25:                                                 ...n 5 minutos de retrso... Feliz Cumple...      NEU, 26:                                                 ...o no dije que jessic se provechr de m...        N, 34:                                                 ...o no dije que jessic se provechr de m...        N, 37:                                                 ...o no dije que jessic se provechr de m...        N, 39:                                                 ...o no dije que jessic se provechr de m...        N, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, '_':  1    0.484784\n-1    0.393641\n 0    0.121575\nName: polarity_bin, dtype: float64, '_15':                                                 ...o que estoy viendo a los Reyes... Per...      NEU, '_16':                                                 ...y me tiran a la basura\" http://t.co/j...        N, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"base_location = '../../ml-football-tweets/'\\n\\n# T..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", \"base_location = './ml-football-tweets/'\\n\\n# To re..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"base_location = './'\\n\\n# To read\\ntweets_raw_file ..._tweets_csv = base_location + 'corpus_tweets.csv'\", 'import pandas as pd\\nimport numpy as np', \"tweets_raw = pd.read_csv(tweets_raw_file, encodi...('Evaluated tweets so far: %d' % len(tweets_run))\", 'def convert_to_numeric(evaluation):\\n    if evalu...\\n        return -1\\n    else:\\n        return 2    ', \"# Build dictionary of tweets where key is the ta...t tweets evalauted so far: %d' % len(own_tweets))\", 'from lxml import objectify', \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2017_csv, index=False, encoding='utf-8')\", \"# 4 values of sentiment: N, P, NONE, NEU\\nxml = o...s_tweets_2012_csv, index=False, encoding='utf-8')\", 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', 'tweets_corpus = pd.concat([\\n        general_twee...corpus_train_2017\\n    ])\\ntweets_corpus.sample(10)', \"print('Total corpus tweets: %d' % len(tweets_corpus))\", 'tweets_corpus = tweets_corpus.query(\\'polarity != \"NONE\"\\')', \"tweets_corpus = tweets_corpus[-tweets_corpus.content.str.contains('^http.*$')]\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'Out': {15:                                                 ...o que estoy viendo a los Reyes... Per...      NEU, 16:                                                 ...y me tiran a la basura\" http://t.co/j...        N, 21:                                                 ...n irresponsble         P\n\n[1680 rows x 2 columns], 22:                                                 ...trle derecho  ...        N\n\n[94 rows x 2 columns], 24:                                               co...              #progrmscmbidos cc d juste        N, 25:                                                 ...n 5 minutos de retrso... Feliz Cumple...      NEU, 26:                                                 ...o no dije que jessic se provechr de m...        N, 34:                                                 ...o no dije que jessic se provechr de m...        N, 37:                                                 ...o no dije que jessic se provechr de m...        N, 39:                                                 ...o no dije que jessic se provechr de m...        N, ...}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, '_':  1    0.484784\n-1    0.393641\n 0    0.121575\nName: polarity_bin, dtype: float64, '_15':                                                 ...o que estoy viendo a los Reyes... Per...      NEU, '_16':                                                 ...y me tiran a la basura\" http://t.co/j...        N, ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/david.santosg/ml-football-tweets/<ipython-input-64-bbf023576d52> in <module>()\n     22     'cls__max_iter': (500, 1000)\n     23 }\n     24 \n     25 \n     26 grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n---> 27 grid_search.fit(tweets_corpus_no_links.content, tweets_corpus_no_links.polarity_bin)\n     28 \n     29 \n     30 \n     31 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ..._score=True,\n       scoring='roc_auc', verbose=0), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...score=True,\n       scoring='roc_auc', verbose=0)>\n        X = 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object\n        y = 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        groups = None\n        self.param_grid = {'cls__C': (0.2, 0.5, 0.7), 'cls__loss': ('hinge', 'squared_hinge'), 'cls__max_iter': (500, 1000), 'vect__max_df': (0.5, 1.9), 'vect__max_features': (500, 1000), 'vect__min_df': (10, 20, 50), 'vect__ngram_range': ((1, 1), (1, 2))}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ..._score=True,\n       scoring='roc_auc', verbose=0), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nLookupError                                        Thu May 17 16:49:58 2018\nPID: 6995                            Python 3.6.0: /opt/Conda.io/bin/python\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), 1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, 1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, make_scorer(roc_auc_score, needs_threshold=True), array([2010, 2014, 2017, ..., 6602, 6603, 6604]), array([   0,    1,    2, ..., 2417, 2419, 2445]), 0, {'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=1       @PauladeLasHeras No te libraras de ayuda...y tan irresponsable \nName: content, dtype: object, y=1       0\n2       1\n3      -1\n4       1\n6       ...6    1\n1007    1\nName: polarity_bin, dtype: int64, scorer=make_scorer(roc_auc_score, needs_threshold=True), train=array([2010, 2014, 2017, ..., 6602, 6603, 6604]), test=array([   0,    1,    2, ..., 2417, 2419, 2445]), verbose=0, parameters={'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...2', random_state=None, tol=0.0001, verbose=0))])>\n        X_train = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y_train = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    263         Returns\n    264         -------\n    265         self : Pipeline\n    266             This estimator\n    267         \"\"\"\n--> 268         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(steps=[(...2', random_state=None, tol=0.0001, verbose=0))])>\n        X = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n    269         if self._final_estimator is not None:\n    270             self._final_estimator.fit(Xt, y, **fit_params)\n    271         return self\n    272 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(steps=[('vect', CountVectorizer(analyze...l2', random_state=None, tol=0.0001, verbose=0))]), X=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64, **fit_params={})\n    229         Xt = X\n    230         for name, transform in self.steps[:-1]:\n    231             if transform is None:\n    232                 pass\n    233             elif hasattr(transform, \"fit_transform\"):\n--> 234                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = 2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object\n        transform.fit_transform = <bound method CountVectorizer.fit_transform of C...on tokenize at 0x7fb115ae6e18>, vocabulary=None)>\n        y = 2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64\n        fit_params_steps = {'cls': {}, 'vect': {}}\n        name = 'vect'\n    235             else:\n    236                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    237                               .transform(Xt)\n    238         if self._final_estimator is None:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, y=2584    1\n2588    1\n2593    1\n2594    1\n2599    ...6    1\n1007    1\nName: polarity_bin, dtype: int64)\n    834         max_df = self.max_df\n    835         min_df = self.min_df\n    836         max_features = self.max_features\n    837 \n    838         vocabulary, X = self._count_vocab(raw_documents,\n--> 839                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    840 \n    841         if self.binary:\n    842             X.data.fill(1)\n    843 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...ion tokenize at 0x7fb115ae6e18>, vocabulary=None), raw_documents=2584    La radio pública cumple 75 años. Enhorab...y tan irresponsable \nName: content, dtype: object, fixed_vocab=False)\n    757         indptr = _make_int_array()\n    758         values = _make_int_array()\n    759         indptr.append(0)\n    760         for doc in raw_documents:\n    761             feature_counter = {}\n--> 762             for feature in analyze(doc):\n        feature = undefined\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    763                 try:\n    764                     feature_idx = vocabulary[feature]\n    765                     if feature_idx not in feature_counter:\n    766                         feature_counter[feature_idx] = 1\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in <lambda>(doc='La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne')\n    236         elif self.analyzer == 'word':\n    237             stop_words = self.get_stop_words()\n    238             tokenize = self.build_tokenizer()\n    239 \n    240             return lambda doc: self._word_ngrams(\n--> 241                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = 'La radio pública cumple 75 años. Enhorabuena a t.... Es más necesaria que nunca. #75añosRNE @EDCHrne'\n    242 \n    243         else:\n    244             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    245                              self.analyzer)\n\n...........................................................................\n/home/david.santosg/ml-football-tweets/<ipython-input-59-ba382de6c55a> in tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne')\n     12 \n     13 def tokenize(text):\n     14     # remove non letters\n     15     text = ''.join([c for c in text if c not in non_words])\n     16     # tokenize\n---> 17     tokens =  word_tokenize(text)\n     18 \n     19     # stem\n     20     try:\n     21         stems = stem_tokens(tokens, stemmer)\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in word_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n    104     for the specified language).\n    105 \n    106     :param text: text to split into sentences\n    107     :param language: the model name in the Punkt corpus\n    108     \"\"\"\n--> 109     return [token for sent in sent_tokenize(text, language)\n        text = 'la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne'\n        language = 'english'\n    110             for token in _treebank_word_tokenize(sent)]\n    111 \n    112 \n    113 \n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py in sent_tokenize(text='la radio pública cumple  años enhorabuena a toda...osible es más necesaria que nunca añosrne edchrne', language='english')\n     88     for the specified language).\n     89 \n     90     :param text: text to split into sentences\n     91     :param language: the model name in the Punkt corpus\n     92     \"\"\"\n---> 93     tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n        tokenizer = undefined\n        language = 'english'\n     94     return tokenizer.tokenize(text)\n     95 \n     96 # Standard word tokenizer.\n     97 _treebank_word_tokenize = TreebankWordTokenizer().tokenize\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in load(resource_url='nltk:tokenizers/punkt/PY3/english.pickle', format='pickle', cache=True, verbose=False, logic_parser=None, fstruct_reader=None, encoding=None)\n    803     # Let the user know what's going on.\n    804     if verbose:\n    805         print('<<Loading %s>>' % (resource_url,))\n    806 \n    807     # Load the resource.\n--> 808     opened_resource = _open(resource_url)\n        opened_resource = undefined\n        resource_url = 'nltk:tokenizers/punkt/PY3/english.pickle'\n    809 \n    810     if format == 'raw':\n    811         resource_val = opened_resource.read()\n    812     elif format == 'pickle':\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in _open(resource_url='nltk:tokenizers/punkt/PY3/english.pickle')\n    921     \"\"\"\n    922     resource_url = normalize_resource_url(resource_url)\n    923     protocol, path_ = split_resource_url(resource_url)\n    924 \n    925     if protocol is None or protocol.lower() == 'nltk':\n--> 926         return find(path_, path + ['']).open()\n        path_ = 'tokenizers/punkt/PY3/english.pickle'\n    927     elif protocol.lower() == 'file':\n    928         # urllib might not use mode='rb', so handle this one ourselves:\n    929         return find(path_, ['']).open()\n    930     else:\n\n...........................................................................\n/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py in find(resource_name='tokenizers/punkt/PY3/english.pickle', paths=['/home/david.santosg/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', ''])\n    643         (resource_name,), initial_indent='  ', subsequent_indent='  ',\n    644         width=66)\n    645     msg += '\\n  Searched in:' + ''.join('\\n    - %r' % d for d in paths)\n    646     sep = '*' * 70\n    647     resource_not_found = '\\n%s\\n%s\\n%s' % (sep, msg, sep)\n--> 648     raise LookupError(resource_not_found)\n        resource_not_found = '\\n***********************************************...*************************************************'\n    649 \n    650 \n    651 def retrieve(resource_url, filename=None, verbose=True):\n    652     \"\"\"\n\nLookupError: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search.fit(tweets_corpus_no_links.content, tweets_corpus_no_links.polarity_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-6cf49ed3e726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mbest_params_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cv_results_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_index_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid_search.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(grid_search, 'grid_search.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-59b881b0a033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcorpus_data_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mcorpus_data_features_nd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_data_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-ba382de6c55a>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/david.santosg/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LinearSVC(C=.2, loss='squared_hinge',max_iter=1000,multi_class='ovr',\n",
    "              random_state=None,\n",
    "              penalty='l2',\n",
    "              tol=0.0001\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = spanish_stopwords,\n",
    "    min_df = 50,\n",
    "    max_df = 1.9,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(tweets_corpus.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_data_features_nd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-c965824ba0cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m scores = cross_val_score(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcorpus_data_features_nd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_bin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_data_features_nd' is not defined"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(tweets_corpus)],\n",
    "    y=tweets_corpus.polarity_bin,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'tweets_parsed.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-0eeeddc5b233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets_parsed.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Conda.io/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'tweets_parsed.csv' does not exist"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweets_parsed.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-8dcad90207c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tweet'"
     ]
    }
   ],
   "source": [
    "tweets = tweets[tweets.tweet.str.len() < 150]\n",
    "tweets.lat = pd.to_numeric(tweets.lat, errors='coerce')\n",
    "tweets = tweets[tweets.lat.notnull()]\n",
    "\n",
    "\n",
    "#We make sure only those tweets in the Murcia bounding box are kept\n",
    "min_lon = -1.157420\n",
    "max_lon = -1.081202\n",
    "min_lat = 37.951741\n",
    "max_lat = 38.029126\n",
    "\n",
    "tweets = tweets[(tweets.lat.notnull()) & (tweets.lon.notnull())]\n",
    "\n",
    "tweets = tweets[(tweets.lon > min_lon) & (tweets.lon < max_lon) & (tweets.lat > min_lat) & (tweets.lat < max_lat)]\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
